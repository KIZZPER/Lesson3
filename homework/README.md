
---

# Результаты экспериментов

## 1. Эксперименты с глубиной сети

**Файл:** `homework_depth_experiments.py`

### 1.1 Базовое сравнение глубины

Сравнение архитектур с разным количеством слоев:

| Архитектура | Количество слоев | Параметры | Test Accuracy |
|-------------|------------------|---------|----------|
| Shallow | 1 | 100K    | 95-96% |
| Medium-1 | 2 | 200K    | 97-98% |
| Medium-2 | 3 | 300K    | 97.5-98% |
| Deep | 4 | 400K    | 97-98% |
| Very Deep | 5 |500K     | 96-97% |

**Графики:**
- `plots/depth_experiments/depth_comparison_test_acc.png` - динамика точности
- `plots/depth_experiments/depth_bar_comparison.png` - итоговое сравнение
- `plots/depth_experiments/parameters_vs_accuracy.png` - зависимость от параметров

### 1.2 Детальный анализ глубины

Исследование архитектур от 1 до 8 слоев с фиксированной шириной.

**Ключевые наблюдения:**
- Оптимальная глубина: 3-4 слоя
- После 5 слоев начинается degradation problem
- Глубокие сети требуют дополнительной регуляризации

### 1.3 Анализ активационных функций

Сравнение ReLU, LeakyReLU и ELU на глубоких сетях.

**Результаты:**
- ReLU: стандартный выбор, хорошая производительность
- LeakyReLU: немного лучше на очень глубоких сетях
- ELU: самая стабильная, но медленнее обучается

---

## 2. Эксперименты с шириной сети

**Файл:** `homework_width_experiments.py`

### 2.1 Сравнение ширины слоев

Тестирование различных конфигураций ширины:

| Архитектура | Скрытые нейроны | Параметры | Test Accuracy |
|-------------|----------|------|----------|
| Tiny | 64->32->16 | 50K | 95-96% |
| Small | 128->64->32 | 100K | 96-97% |
| Medium | 256->128->64 | 200K | 97-98% |
| Large | 512->256->128 | 400K | 97.5-98% |
| Extra Large | 1024->512->256 | 1M | 97-98% |

**Графики:**
- `plots/width_experiments/width_comparison.png`
- `plots/width_experiments/width_vs_params.png`
- `plots/width_experiments/training_time_vs_width.png`

### 2.2 Оптимальная ширина

Grid search по различным комбинациям ширины слоев.

**Findings:**
- Оптимальная конфигурация: 256->128->64
- Соотношение параметры/качество достигает плато после 400K параметров
- Очень широкие сети склонны к переобучению без регуляризации

### 2.3 Bottleneck архитектуры

Исследование архитектур с узким средним слоем:
- **Expanding:** 128->256->512 
- **Contracting:** 512->256->128 
- **Bottleneck:** 256->64->256 

**Результат:** Contracting архитектуры показывают лучшие результаты.

---

## 3. Эксперименты с регуляризацией

**Файл:** `homework_regularization_experiments.py`

### 3.1 Сравнение техник регуляризации

| Техника | Описание | Test Accuracy | Overfitting Gap |
|---------|----------|-------|---------|
| No Regularization | Baseline | 97% | 3-4% |
| Dropout (0.1) | Легкий dropout | 97.2% | 2.5% |
| Dropout (0.3) | Средний dropout | 97.5% | 2% |
| Dropout (0.5) | Сильный dropout | 96.8% | 1.5% |
| BatchNorm | Нормализация батчей | 97.8% | 1.8% |
| Dropout + BN | Комбинация | 98% | 1.5% |
| L2 (0.0001) | Слабая L2 | 97.3% | 2.5% |
| L2 (0.001) | Сильная L2 | 97% | 2% |

**Графики:**
- `plots/regularization_experiments/all_techniques_test_acc.png`
- `plots/regularization_experiments/techniques_bar_comparison.png`
- Индивидуальные графики overfitting для каждой техники

### 3.2 Адаптивная регуляризация

Комбинированные подходы с разной силой регуляризации:

**Конфигурации:**
1. **Light Regularization:** Dropout(0.2) + BatchNorm + L2(0.00001)
2. **Medium Regularization:** Dropout(0.3) + BatchNorm + L2(0.0001)
3. **Heavy Regularization:** Dropout(0.5) + BatchNorm + L2(0.001)

**Лучший результат:** Medium regularization (98% accuracy)

### 3.3 Анализ весов

Визуализация распределения весов с различными техниками регуляризации:
- `plots/regularization_experiments/weights_no_reg.png`
- `plots/regularization_experiments/weights_dropout.png`
- `plots/regularization_experiments/weights_l2.png`

---

# Итог

## Лучшие конфигурации

### 3 модели по точности:

1. **Medium with Regularization**
   - Архитектура: 256->128->64
   - Регуляризация: Dropout(0.3) + BatchNorm
   - Test Accuracy: 98.0%
   - Параметры: 220K
   - Время обучения: 5 мин (20 эпох)

2. **Deep with BatchNorm**
   - Архитектура: 512->256->128->64
   - Регуляризация: BatchNorm
   - Test Accuracy: 97.8%
   - Параметры: 450K
   - Время обучения: 8 мин (20 эпох)

3. **Wide with Dropout**
   - Архитектура: 512->256->128
   - Регуляризация: Dropout(0.3)
   - Test Accuracy: 97.5%
   - Параметры: 400K
   - Время обучения: 7 мин (20 эпох)


---

# Выводы

## Глубина сети

1. Оптимальная глубина: 3-4 скрытых слоя
2. Сети глубже 5 слоев страдают от degradation без skip connections
3. Активационные функции: ReLU достаточно для средней глубины
4. Глубокие сети требуют BatchNorm для стабильного обучения

## Ширина сети

1. Оптимальная ширина: 256→128→64
2. Sweet spot: 200-400K параметров
3. Очень широкие сети не дают существенного прироста
4. Сужающиеся архитектуры (contracting) работают лучше расширяющихся

## Регуляризация

1. Лучшая техника: Dropout(0.3) + BatchNorm
2. BatchNorm обеспечивает стабильность и небольшую регуляризацию
3. Dropout эффективен против переобучения, но 0.5 слишком много
4. L2 регуляризация менее эффективна, чем Dropout+BN
5. Комбинированный подход дает лучший баланс

